{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import keras\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function get 4 inputs\n",
    "* `data`: this is the train pics which is gonna get augmented.\n",
    "* `label`: labels of those train pics\n",
    "* `augmentation_num`: number of times which we wanna pics get augmented.\n",
    "* `batch_size`: we wanna batch our datas so we can give them batch by batch it this make us able to get pic f.e if we set our `batch_size` 32 we append our pics in batches with 32 pics in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes pics and its labels and divide it into batches and augment each batch\n",
    "def pic_augmentation(data , label , augmentation_num , batch_size):\n",
    "    length = len(label)\n",
    "    ultimate_list = []\n",
    "    list_of_pic_x = []\n",
    "    list_of_pic_y = []\n",
    "    for index in range(length):\n",
    "        pic = data[index]\n",
    "        y = label[index]\n",
    "        for num in range(augmentation_num):\n",
    "            Seed = (num , 0)\n",
    "            pic = tf.image.stateless_random_brightness(pic , max_delta = 0.3 , seed = Seed)\n",
    "            pic = tf.image.stateless_random_contrast(pic , lower = 0.1 , upper = 0.9 , seed = Seed)\n",
    "            pic = tf.image.stateless_random_flip_left_right(pic , seed = Seed)\n",
    "            augmented_pic = tf.image.stateless_random_flip_up_down(pic , seed = Seed)\n",
    "            # tf.image.stateless_random_jpeg_quality(pic,?)\n",
    "            # tf.image.stateless_random_crop(pic,?)\n",
    "            # tf.image.stateless_random_saturation(pic)\n",
    "            # tf.image.stateless_random_hue(pic)\n",
    "            # tf.image.stateless_sample_distorted_bounding_box(pic)\n",
    "            data.append(augmented_pic)\n",
    "            label.append(y)\n",
    "    shuffled_data , shuffled_label = shuffle(data , label)\n",
    "    for data , label in zip(shuffled_data , shuffled_label):\n",
    "        list_of_pic_x.append(data)\n",
    "        list_of_pic_y.append(label)\n",
    "        if len(list_of_pic_x) == batch_size:\n",
    "            ultimate_list.append((np.array(list_of_pic_x) , np.array(list_of_pic_y)))\n",
    "            list_of_pic_x = []\n",
    "            list_of_pic_y = []\n",
    "    return ultimate_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function give us total number of pics in every folder(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num of all pics\n",
    "def get_len(the_path):\n",
    "    length = 0\n",
    "    categories = os.listdir(the_path)\n",
    "    for category in categories:\n",
    "        Path = os.path.join(the_path , category)\n",
    "        length = length + len(os.listdir(Path))\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function take these 4 inputs:\n",
    "* `Batch`: f.e we have 200 batches in every epoch. our `Batch` will be in range of 1-200. its indicate our batch number\n",
    "* `Big_Batch_size`: this variable indicates number of *paths of pics* into a list. f.e we have 1000 pics to train on. if this variable be 250 we will load **paths** of 250 pics into a list\n",
    "* `the_path`: the path which we have our pics in\n",
    "* `Batch_size`: our batch size which we choose for our training process\n",
    "\n",
    "and after loading we shuffle the list and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path of a big batch of images that we will load portion of it into ram using the paths in big batch\n",
    "def get_image_big_batch(Batch , Big_Batch_size , the_path , Batch_size):\n",
    "    categories = os.listdir(the_path)\n",
    "    Start = Big_Batch_size * (Batch // ((Big_Batch_size * len(categories)) // Batch_size))\n",
    "    categories = os.listdir(the_path)\n",
    "    MyDataSet = []\n",
    "    for category in categories:\n",
    "        label = categories.index(category)\n",
    "        Path = os.path.join(the_path , category)\n",
    "        if len(os.listdir(Path)) > Start + Big_Batch_size:\n",
    "            for pic_name in os.listdir(Path)[Start : Start + Big_Batch_size]:\n",
    "                MyDataSet.append((os.path.join(Path , pic_name) , label))\n",
    "        else:\n",
    "            for pic_name in os.listdir(Path)[Start : ]:\n",
    "                MyDataSet.append((os.path.join(Path , pic_name) , label))\n",
    "    shuffled_MyDataSet = shuffle(MyDataSet)\n",
    "    return shuffled_MyDataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function load a part of Big Batch into our ram. the whole process is that we load a list of pics after that we load portion of it into our ram for training. this function take 4 inputs:\n",
    "* `batch`: f.e we have 200 batches in every epoch. our `Batch` will be in range of 1-200. its indicate our batch number\n",
    "* `portion_size`: the size of portion we want to load. `portion_size`= 10 we load 10 pics from our Big Batch\n",
    "* `the_big_batch`: the Big Batch to load from\n",
    "* `pic_size`: indicates the size of our pic. must be a tuple \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a portion of pics into ram\n",
    "def get_portion_from_big_batch(batch , portion_size , the_big_batch , pic_size: tuple[int , int]):\n",
    "    Start = portion_size * (batch % (len(the_big_batch) // portion_size))\n",
    "    X_portion_of_MyDataSet = []\n",
    "    Y_portion_of_MyDataSet = []\n",
    "    for path , Y in the_big_batch[Start : Start + portion_size]:\n",
    "        try:\n",
    "            image = cv2.imread(path)\n",
    "            resized = cv2.resize(image , pic_size)\n",
    "            X_portion_of_MyDataSet.append(resized)\n",
    "            Y_portion_of_MyDataSet.append(Y)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return X_portion_of_MyDataSet , Y_portion_of_MyDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(the_path , batch_size , portion_size , Big_Batch_size , n_epoch , pic_size: tuple[int , int] , model , augmentation_num = 0):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    categories = os.listdir(the_path)\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch in tqdm(range((get_len(the_path)) // batch_size)):\n",
    "            if batch % (((Big_Batch_size * len(categories)) // batch_size)) == 0:\n",
    "                access_1 = 'granted'\n",
    "            else:\n",
    "                access_1 = 'not granted'\n",
    "            if access_1 == 'granted':\n",
    "                Big_Batch = get_image_big_batch(batch , Big_Batch_size , the_path , batch_size)   \n",
    "            portion_x , portion_y = get_portion_from_big_batch(batch , portion_size , Big_Batch , pic_size)\n",
    "            list_of_augmented_pic = zip(portion_x , portion_y)\n",
    "            if augmentation_num > 0:\n",
    "                list_of_augmented_pic = pic_augmentation(portion_x , portion_y , augmentation_num , batch_size)\n",
    "            for batch_x , batch_y in list_of_augmented_pic:\n",
    "                loss , acc = model.train_on_batch(np.array(batch_x) , np.array(batch_y))\n",
    "                loss_history.append(loss)\n",
    "                acc_history.append(acc)\n",
    "        print('Epoch: %d ,     Train Loss %.4f,    Train Acc. %.4f' % (epoch+1 , loss , acc))\n",
    "    return {'loss_history' : loss_history , 'acc_history' : acc_history}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from sklearn.metrics import f1_score,accuracy_score,confusion_matrix,mean_squared_error,r2_score,mean_absolute_error\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(the_path):\n",
    "    length=0\n",
    "    categories=os.listdir(the_path)\n",
    "    for category in categories:\n",
    "        Path=os.path.join(the_path,category)\n",
    "        length=length+len(os.listdir(Path))\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and resizing the images\n",
    "def get_image_big_batch(Batch,Big_Batch_size,the_path,Batch_size,Random_state):\n",
    "    Start=Big_Batch_size*(Batch//((Big_Batch_size*2)//Batch_size))\n",
    "    categories=os.listdir(the_path)\n",
    "    MyDataSet=[]\n",
    "    for category in categories:\n",
    "        label=categories.index(category)\n",
    "        Path=os.path.join(the_path,category)\n",
    "        if len(os.listdir(Path))>Start+Big_Batch_size:\n",
    "            for pic_name in os.listdir(Path)[Start:Start+Big_Batch_size]:\n",
    "                MyDataSet.append((os.path.join(Path,pic_name),label))\n",
    "        else:\n",
    "            for pic_name in os.listdir(Path)[Start:]:\n",
    "                MyDataSet.append((os.path.join(Path,pic_name),label))\n",
    "    shuffled_MyDataSet=shuffle(MyDataSet,random_state=Random_state)\n",
    "    return shuffled_MyDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_from_big_batch(batch,batch_size,the_big_batch,pic_size):\n",
    "    Start=batch_size*(batch%(len(the_big_batch)//batch_size))\n",
    "    mini_X_MyDataSet=[]\n",
    "    mini_Y_MyDataSet=[]\n",
    "    for path,Y in the_big_batch[Start:Start+batch_size]:\n",
    "        try:\n",
    "            image=cv2.imread(path)/255\n",
    "            resized = cv2.resize(image, (pic_size, pic_size))\n",
    "            mini_X_MyDataSet.append(resized)\n",
    "            mini_Y_MyDataSet.append(Y)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return mini_X_MyDataSet,mini_Y_MyDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(the_path,batch_size,Big_Batch_size,n_epoch,val_num_in_batch,pic_size,seed):\n",
    "    loss_history=[]\n",
    "    acc_history=[]\n",
    "    val_loss_history=[]\n",
    "    val_acc_history=[]\n",
    "    for epoch in range(n_epoch):\n",
    "        # val_xds.clear()\n",
    "        # val_yds.clear()\n",
    "        val_xds=[]\n",
    "        val_yds=[]\n",
    "        for batch in tqdm(range(get_len(the_path)//batch_size)):\n",
    "            if batch%(batch//((Big_Batch_size*2)//batch_size))==0:\n",
    "                access='granted'\n",
    "            else:\n",
    "                access='not granted'\n",
    "            if access=='granted':\n",
    "                Big_Batch=get_image_big_batch(batch,Big_Batch_size,the_path,batch_size,Random_state=seed)\n",
    "            batch_x,batch_y=get_batch_from_big_batch(batch,batch_size,Big_Batch,pic_size)\n",
    "            for val_element in batch_x[-val_num_in_batch:]:\n",
    "                val_xds.append(val_element)\n",
    "            for val_element in batch_y[-val_num_in_batch:]:\n",
    "                val_yds.append(val_element)\n",
    "            loss,acc=model.train_on_batch(np.array(batch_x[:batch_size-val_num_in_batch]),np.array(batch_y[:batch_size-val_num_in_batch]))\n",
    "            # val_loss,val_acc=model.test_on_batch(np.array(batch_x[-val_num_in_batch:]),np.array(batch_y[-val_num_in_batch:]))\n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(acc)\n",
    "        val_loss,val_acc=model.evaluate(np.array(val_xds),np.array(val_yds))\n",
    "        del val_xds[:]\n",
    "        del val_xds\n",
    "        del val_yds[:]\n",
    "        del val_yds\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        print('Epoch: %d ,     Train Loss %.4f,    Train Acc. %.4f,    Val Loss %.4f,    Val Acc. %.4f' %\n",
    "\t\t\t(epoch+1, loss, acc, val_loss, val_acc))\n",
    "    return {'loss_history':loss_history,'acc_history':acc_history,'val_loss_history':val_loss_history,'val_acc_history':val_acc_history}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
